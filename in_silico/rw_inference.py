import numpy as np
from scipy.optimize import minimize
import time
import sys

sys.path.append('/home/ed/Documents/Academic/Edinburgh/Courses/Dissertation/LeukocyteMigration')

from utils.angles import angle_between
from utils.parallel import parallel_methods
from in_silico.walkers import reference_axis
from utils.distributions import WrappedNormal
from in_silico.sources import Source
from utils.exceptions import OptimisationError

class BPbayesian:

    def __init__(self, paths: np.array, source: Source):
        """
        Parameters
        ----------
        paths       np.array - (T+1, 2, N) - paths of N walkers walking for T timesteps.
                    Not every path must be of the same length, but the longest should be
                    T+1. For every other path i, start at index 0 and fill up to length t
                    [:t, :, i] leaving [t:, :, i] filled with np.nan values.
        source      a sources.Source class, with a direction_to_source function

        """

        self.paths = paths
        self.source = source

        self.alphas, self.betas = self._convert_path(paths)
        self.alpha0, self.beta0 = self.alphas[0, :], self.betas[0, :]
        self.betas = self.betas[1:, :]
        alphas = self.alphas[1:, :]
        alpha_prevs = self.alphas[:-1, :]
        self.alphas, self.alphas_ = alphas, alpha_prevs

        self.n_paths = self.alphas.shape[1]

    def _convert_path(self, paths: np.array):
        """
        Given a set of raw x-y coordinates, get a list of the alphas and betas

        Params:

            paths:     np.array - (T+1, 2, N) - paths of N walkers walking for T timesteps

        Returns:

            alphas:    np.array - (T, N): angle taken at each time step
            betas:     np.array - (T, N): direction to source at each time step

        """

        moves = paths[1:, :, :] - paths[:-1, :, :]
        alphas = np.apply_along_axis(lambda move: angle_between(reference_axis, move), 1, moves)
        directions_to_source = np.apply_along_axis(self.source.direction_to_source, 1, paths)
        betas = np.apply_along_axis(lambda d: angle_between(reference_axis, d), 1, directions_to_source)[:-1, :]

        return alphas, betas

    def log_likelihood(self, w: float, p: float, b: float) -> float:
        """
        Get the log-likelihood of a set of parameters w, p, b.

        * 'alpha' refers to the current observed angle under consideration.
        * 'beta' refers to the current angle towards the source
        * 'alpha_prev' refers to the previous observed angle

        at any step t > 1, the probability distribution governing the observed
        angle alpha is

             p(alpha) = w * WN(beta, -2log(b)) + (1-w) * WN(alpha_prev, -2log(p))

        where WN(mu, sig) is a wrapped normal distribution. The log likelihood
        of observing T strings of angles 'alphas' is

            SUM_t SUM_i log[p(alpha_ti)]

        Params:

            w:         The variable w which defines the probability of taking a biased step
            p:         The variable p, defining the variance of persistent motion
            b:         The variable b, defining the variance of boased motion

        Returns:

            log_prob:  The log probability that the paths were generated by a walker
                       with parameters (w, p, b)
        """

        sig_b = -2 * np.log(b)
        sig_p = -2 * np.log(p)

        # The probability of observing the first step, given the angle beta0 towards the source
        p_0 = WrappedNormal(mu=self.beta0, sig=sig_b).pdf(x=self.alpha0)

        # biased probabilities
        p_b = WrappedNormal(mu=self.betas, sig=sig_b).pdf(x=self.alphas)

        # persistent probabilities
        p_p = WrappedNormal(mu=self.alphas_, sig=sig_p).pdf(x=self.alphas)

        # combined probabilities
        p_t = w * p_b + (1 - w) * p_p

        log_p_0 = np.log(p_0)
        log_p_t = np.log(p_t)

        # this handles for when each path has an uneven number of steps. Some values will be nan
        log_p_t[np.isnan(log_p_t)] = 0
        log_p_0[np.isnan(log_p_0)] = 0

        log_prob = log_p_0.sum() + log_p_t.sum()

        return log_prob

    def infer(self, w0: float, p0: float, b0: float,
              n_steps: int=10000, burn_in: int=3000, seed: int=0) -> np.array:
        """
        Given a set of starting parameters, perform MCMC bayesian inference for n_steps with
        a burn in of burn_in and a maximum step size in each direction of step_size

        Params:

            w0:        The starting point for w
            p0:        The starting point for p
            b0:        The starting point for b
            step:      Each new point is proposed as x' = x + uniform(-step, step) in each dimension
            n_steps:   The total number of MCMC steps to perform, after burn in
            burn_in:   The total number of burn in steps
            seed:      The number to to seed all random MC operations

        Returns:

            params:    np.array, shape: (n_steps, 3) -> the distribution over parameters

        """

        np.random.seed(seed)

        n = n_steps + burn_in
        params = []
        w, p, b = w0, p0, b0
        L0 = self.log_likelihood(w, p, b)
        step = [0.02, 0.02, 0.02]
        t0 = time.time()
        total_accepts = 0
        rolling_accepts = 0
        rolling_rejects = 0

        for i in range(n):

            # add random purturbation to w, p, b
            dw, dp, db = np.random.normal(0, step)
            w_, p_, b_ = w + dw, p + dp, b + db

            # If any param is outside of the range [0, 1], reject the step
            if any([param > 1 or param < 0 for param in [w_, b_, p_]]):
                params.append([w, p, b])
                rolling_rejects += 1
                continue

            # evaluate the log-likelihood of our proposed move
            L_p = self.log_likelihood(w_, p_, b_)

            # evaluate the probability of moving
            prob = np.exp(L_p - L0)

            # decide whether to move
            if np.random.uniform(0, 1) < prob:
                w, p, b = w_, p_, b_
                L0 = L_p
                total_accepts += 1
                rolling_accepts += 1
                rolling_rejects = 0
            else:
                rolling_rejects += 1

            if rolling_rejects == 100:
                print(' WARNING: {} simultaneous rejections'.format(100))
                rolling_rejects = 0

            # append w, p, b regardless of whether the step was accepted or not
            params.append([w, p, b])

            # adapt the guassian kernel width to be half the standard deviation of previous steps
            if i % 100 == 99:
                step = np.array(params).std(0) / 2
                t = time.time() - t0
                print('\r' + 'Estimated time remaining : {:.0f}s. Total acceptance Rate: {:.3f}. Rolling acceptance rate: {:.3f}'.format(t * n / i - t, total_accepts / i, rolling_accepts / 100), end='')
                rolling_accepts = 0

        return np.array(params)[burn_in:, :]

    def multi_infer(self, n_walkers: int=5, n_steps: int=10000,
                    burn_in: int=3000, sneaky_init: bool=True, seed: int=0):
        '''
        Use multiple MCMC walkers in parallel

        Params:

            n_walkers:     The number of independant MCMC walkers to use in parallel
            step:          Each new point is proposed as x' = x + uniform(-step, step) in each dimension
            n_steps:       The total number of MCMC steps to perform, after burn in
            burn_in:       The total number of burn in steps
            sneaky_init:   If true, use scipy.optimize.minimize to find the most probable place in
                           (w, p, b) space and start MCMC walk here, instead of using a burn in.

        Returns:

            params:        np.array, shape: (n_walkers * n_steps, 3) -> the distribution over parameters

        '''

        np.random.seed(seed)

        objs = [self for _ in range(n_walkers)]
        methods = ['infer' for _ in range(n_walkers)]

        if sneaky_init:

            try:
                w0, p0, b0 = self.most_probable()
                burn_in = 0
                print('Using sneaky init: burn in will be set to zero')

            except OptimisationError as e:
                w0, p0, b0 = np.random.uniform(0, 1, size=(3,))
                print('Warning: {}. Initial state will be random, with burn in {}.'.format(e, burn_in))

            print('Starting at: w={:.2f}, p={:.2f}, b={:.2f}'.format(w0, p0, b0))

        else:
            w0, p0, b0 = np.random.uniform(0, 1, size=(3,))

        params = [{'w0': w0, 'p0': p0, 'b0': b0,
                   'n_steps': n_steps, 'burn_in': burn_in, 'seed': i + seed} for i in range(n_walkers)]

        t0 = time.time()
        print('Beginning MCMC walk')
        res = parallel_methods(objs, methods, params, backend='multiprocessing')
        print('Completed MCMC walk in {:.2f}s'.format(time.time() - t0))
        try:
            return np.concatenate(res, axis=0)
        except ValueError:
            print('Warning: one or more of the parallel MCMC walkers failed')
            return np.concatenate([r for r in res if r.shape == (n_steps, 3)], axis=0)

    def most_probable(self, method: str='SLSQP', seed: int=0):
        """

        Use scipy optimiser to find the most probable set of parameters

        Parameters
        ----------
        method          which scipy optimize method to use
        seed            a numpy random seed to use

        Returns
        -------
        best params     a np.array of the most probable (w, p, b) parameters
        """


        np.random.seed(seed)

        z = minimize(lambda x: -self.log_likelihood(*x),
                     x0=np.random.uniform(0, 1, 3),
                     method=method,
                     bounds=[(0.01, 1) for _ in range(3)])

        if not z.success:
            raise OptimisationError('Scipy optimiser failed')
        elif np.all(z.x < 0.98) and np.all(z.x > 0.02):
            return z.x
        else:
            raise OptimisationError('Scipy optimiser found unrealistic values')




if __name__ == '__main__':

    from walkers import BP_Leukocyte
    from sources import PointSource
    import time
    from utils.plotting import plot_wpb_dist

    # np.set_printoptions(precision=6, linewidth=500, threshold=500, suppress=True)
    #
    #
    # n_walkers = 20
    # T = 100
    w = 0.4
    b = 0.6
    p = 0.8
    #
    #
    # X0s = [np.random.uniform(-5, 5, size=(2,)) for i in range(n_walkers)]
    # leukocyte = BP_Leukocyte(w, p, b, source)
    # paths = leukocyte.multi_walk(X0s, T)
    #
    # T, X, W = paths.shape
    #
    # M = np.zeros_like(paths)
    #
    # for walker in range(W):
    #     i = (1 + walker) * 5
    #     M[:i, :, walker] = paths[:i, :, walker]
    #
    # M[M == 0] = np.nan
    #
    # BI = BPbayesian(M, source)
    #
    # t1 = time.time()
    # params = BI.multi_infer(n_walkers=5,
    #                         step=0.02,
    #                         n_steps=10000,
    #                         burn_in=3000,
    #                         sneaky_init=True)
    #
    # plot_wpb_dist(params)
    #
    # t2 = time.time()
    #
    # print('Completed inference in {:.2f}s'.format(t2-t1))

    import pandas as pd

    data = pd.read_csv('/media/ed/DATA/Datasets/Leukocytes/Control wounded 1hr/control 1hour wound Links in tracks statistics.csv')
    stats = pd.read_csv('/media/ed/DATA/Datasets/Leukocytes/Control wounded 1hr/control 1hr wound Track statistics.csv')

    stats['TRACK_START'] = stats['TRACK_START'].astype(int) // 30
    data = data[['TRACK_ID', 'EDGE_X_LOCATION',	'EDGE_Y_LOCATION']]

    paths = []
    for id, path in data.groupby('TRACK_ID'):
        t0 = stats[['TRACK_START', 'TRACK_ID']][stats['TRACK_ID'] == 0]['TRACK_START'].values[0]
        paths.append((t0, path[['EDGE_X_LOCATION',	'EDGE_Y_LOCATION']].values))

    from utils.plotting import make_gif
    import skimage

    frames = skimage.io.imread('/media/ed/DATA/Datasets/Leukocytes/Control wounded 1hr/Pupae 1 concatenated ubi-ecad-GFP, srpGFP; srp-3xH2Amch x white-1HR.tif')[:, 0, :, :]
    # print(frames.shape)

    # make_gif(frames, save_as='ttest', paths=paths,
    #          extent=[0, data['EDGE_X_LOCATION'].max(), 0, data['EDGE_Y_LOCATION'].max() + 12],
    #          origin='lower')
    #

    max_t = max(paths, key=lambda x: x[1].shape[0])[1].shape[0]
    M = np.zeros((max_t, 2, len(paths)))
    M[:] = np.nan

    for i, (t0, path) in enumerate(paths):
        M[:path.shape[0], :, i] = path

    source = PointSource(position=np.array([96, 110]))

    BI = BPbayesian(M, source)

    # t0 = time.time()
    # print(BI.log_likelihood(w, p, b))
    # print('{:.4f}s'.format(time.time() - t0))

    t1 = time.time()
    params = BI.multi_infer(n_walkers=5,
                            step=0.02,
                            n_steps=10000,
                            burn_in=3000,
                            sneaky_init=True)

    plot_wpb_dist(params)

    t2 = time.time()

    print('Completed inference in {:.2f}s'.format(t2-t1))
